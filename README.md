# **Human Action Recognition in Videos using Pedestrian Keypoints**
*Case study in Artificial Intelligence and Visual Computing as part of a [group school project](https://moodle.polytechnique.fr/course/view.php?id=13078)*.

___
___

- [Introduction](#introduction)
- [The Dataset](#the-dataset)

___

## **Introduction**

Classifying pedestrians' every-day life actions is a state-of-the art topic in computer vision, including a lot of sub-topics like pedestrian detection, segmentation, video consolidation, classification, etc.

To recognize an action, a human being would localize the person first, analyze the position of its body-parts, see how these are moving and interacting,    and classify accordingly.

We propose here to reproduce this process, using the information of body-parts position (pedestrian keypoints/skeleton) along pedestrian videos.
The data to process is a sequence of labelled 3D coordinates on pedestrians, and the output is the label of an action (walking, running, biking, falling on the ground, fighting, etc.).

___

## **The Dataset**

The dataset used is the **NTU RGB+D - Action Recognition Dataset** described in [this repository](https://github.com/shahroudy/NTURGB-D).
When cloning the current respository please download the following [zip file](https://drive.google.com/u/0/uc?export=download&confirm=7nHU&id=1CUZnBtYwifVXS21yVg62T-vrPVayso5H) (5.8 Go) and extract it (13.4 Go) in the data folder. This compressed folder contains the relevant data for our project.

### ***Data Analysis***

*TODO*

### ***Data Visualization***

*TODO*

___

## **Models**

___

## **Results**

___

## **Conclusion**

___

### **References**

- [**NTU RGB+D - Action Recognition Dataset**](https://github.com/shahroudy/NTURGB-D)

___
___