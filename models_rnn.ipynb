{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNN Models for Human Action Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the models implemented, trained and analyzed in this notebook:\n",
    "* LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. INITIALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1.1. IMPORT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1.2. DATA LOADING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/nturgb+d_skeletons_cleaned/\"\n",
    "data_files = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {}\n",
    "\n",
    "with open(\"data/actions.txt\", 'r') as actions_file:\n",
    "    for i,line in enumerate(actions_file.readlines()):\n",
    "        actions[i] = line.replace('\\n', '').split('.')[-1][1:]\n",
    "    actions_file.close()\n",
    "\n",
    "classes = np.array(list(actions.keys()))\n",
    "nb_classes = classes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanActionDataset(Dataset):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    dataset matching sequences of 25 joints in a 3D space to action classes \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, data_dir, data_files):\n",
    "\n",
    "        \"\"\"\n",
    "        args:\n",
    "        - data_dir (string): directory with all the arrays (of size sequence_length x 25 x 3 reshaped as sequence_length x 75\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.data_files = data_files\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.Tensor(np.load(self.data_dir + self.data_files[idx]))\n",
    "        tensor = tensor.reshape((tensor.shape[0], 75))\n",
    "        label = int(data_files[idx][17:-4])\n",
    "        return (tensor, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PyTorchâ€™s DataLoader with Variable Length Sequences for LSTM/GRU : from this [article](https://www.codefull.net/2018/11/use-pytorchs-dataloader-with-variable-length-sequences-for-lstm-gru/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequence():\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        # let's assume that each element in \"batch\" is a tuple (data, label).\n",
    "        # sort the batch in the descending order\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "        \n",
    "        # get each sequence and pad it\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "        lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "\n",
    "        # don't forget to grab the labels of the *sorted* batch\n",
    "        labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "        return sequences_padded, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAD = HumanActionDataset(data_dir, data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(HAD, [int(0.80*len(HAD)), len(HAD)-int(0.80*len(HAD))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, collate_fn=PadSequence(), shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=64, collate_fn=PadSequence(), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1.3 AUXILIARY FUNCTIONS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nb_epochs, epoch_print_frequence):\n",
    "\n",
    "    s = time.time()\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        running_loss_train, running_loss_val = 0, 0\n",
    "\n",
    "        for train in [True, False]:\n",
    "\n",
    "            if train:\n",
    "                dataloader = train_dataloader\n",
    "                model.train()\n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()\n",
    "\n",
    "            for data in dataloader:\n",
    "                \n",
    "                inputs = data[0].to(device)\n",
    "                labels = data[-1].to(device)\n",
    "\n",
    "                if train:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss_train += loss.item()\n",
    "                else:\n",
    "                    running_loss_val += loss.item()\n",
    "\n",
    "        running_loss_train /= len(train_dataloader)\n",
    "        running_loss_val /= len(val_dataloader)\n",
    "\n",
    "        train_losses.append(running_loss_train)\n",
    "        val_losses.append(running_loss_val)\n",
    "\n",
    "        if epoch % epoch_print_frequence == 0 and epoch > 0:\n",
    "            print(\"epochs {} ({} s) | train loss : {} | val loss : {}\".format(\n",
    "                epoch,\n",
    "                int(time.time()-s),\n",
    "                int(1000000*running_loss_train)/1000000,\n",
    "                int(1000000*running_loss_val)/1000000\n",
    "            ))\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. THE MODELS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.1. LSTM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM0(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_classes, input_size, hidden_size, num_layers, device):\n",
    "\n",
    "        super(LSTM0, self).__init__()\n",
    "\n",
    "        self.num_classes = nb_classes   # number of classes\n",
    "        self.num_layers = num_layers    # number of layers\n",
    "        self.input_size = input_size    # input size\n",
    "        self.hidden_size = hidden_size  # hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # lstm\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, nb_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # hidden state (short memory)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # internal state (long memory)\n",
    "        \n",
    "        _, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        hn = hn.view(-1, self.hidden_size) # reshaping the data for clasifier\n",
    "        return self.classifier(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM0(\n",
       "  (lstm): LSTM(75, 256, batch_first=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=60, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM0 = LSTM0(nb_classes, input_size=75, hidden_size=256, num_layers=1, device=device)\n",
    "model_LSTM0.to(device)\n",
    "model_LSTM0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_LSTM0 = nn.CrossEntropyLoss()\n",
    "optimizer_LSTM0 = torch.optim.Adam(params=model_LSTM0.parameters(), lr=1e-2)\n",
    "nb_epochs = 10\n",
    "epoch_print_frequence = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17160/2678278595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses_LSTM0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_LSTM0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_LSTM0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_LSTM0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_print_frequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17160/2234239334.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, nb_epochs, epoch_print_frequence)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "losses_LSTM0 = train_model(model_LSTM0, criterion_LSTM0, optimizer_LSTM0, nb_epochs, epoch_print_frequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"LSTM0 loss evolution\")\n",
    "plt.plot(losses_LSTM0[0], label=\"train\")\n",
    "plt.plot(losses_LSTM0[1], label=\"test\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "118508df3531a4baef22a353ae7891686b2f0c3ff24a04c6bb67baeaff15e974"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
